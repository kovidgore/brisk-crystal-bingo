{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment4_2018IMT_021.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name : Arsh Pratap\n",
        "\n",
        "Course : Natural Language Processing\n",
        "\n",
        "Roll No. : 2018IMT-021"
      ],
      "metadata": {
        "id": "bW2qu4vsklXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axZrxiCsUGjx",
        "outputId": "e7601491-795e-43fa-f99a-d12108b9ebba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 23.9 MB/s \n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2022.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.63.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Installing collected packages: regex, nltk\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.7 regex-2022.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeMpZmHlTNSW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "import requests\n",
        "import io\n",
        "from nltk import word_tokenize, sent_tokenize \n",
        "from nltk.lm import MLE\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#   Basic N-gram Language Modelling"
      ],
      "metadata": {
        "id": "cMSpY2kxTS-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [['how', 'are', 'you'], ['i', 'am', 'fine', 'and', 'so', 'is', 'everything']]"
      ],
      "metadata": {
        "id": "8cCfR4HGTTxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Bigrams and trigrams generated---\")\n",
        "print(*list(bigrams(text[0])),sep=\"\\n\")\n",
        "print(*(list(ngrams(text[1], n=3))),sep=\"\\n\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vyazsXiUQzm",
        "outputId": "f2c394d2-b1e6-4cc2-ce14-72992d5c6ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Bigrams and trigrams generated---\n",
            "('how', 'are')\n",
            "('are', 'you')\n",
            "('i', 'am', 'fine')\n",
            "('am', 'fine', 'and')\n",
            "('fine', 'and', 'so')\n",
            "('and', 'so', 'is')\n",
            "('so', 'is', 'everything')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Trigrams with padding symbols---\")\n",
        "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=3))\n",
        "print(*list(ngrams(padded_sent, n=3)),sep=\"\\n\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUlqFiGNUTIR",
        "outputId": "88c57130-4f6d-4d00-fa3e-9c6522af8d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Trigrams with padding symbols---\n",
            "('<s>', '<s>', 'how')\n",
            "('<s>', 'how', 'are')\n",
            "('how', 'are', 'you')\n",
            "('are', 'you', '</s>')\n",
            "('you', '</s>', '</s>')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Generated sample N-grams of max length = 2---\")\n",
        "padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
        "print(*list(everygrams(padded_bigrams, max_len=2)),sep=\"\\n\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgJNiJwKUV5D",
        "outputId": "a83b2697-85c2-4903-ee1d-7cebdf247b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Generated sample N-grams of max length = 2---\n",
            "('<s>',)\n",
            "('<s>', 'how')\n",
            "('how',)\n",
            "('how', 'are')\n",
            "('are',)\n",
            "('are', 'you')\n",
            "('you',)\n",
            "('you', '</s>')\n",
            "('</s>',)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Flattened sentences with padding symbols---\")\n",
        "print(*list(flatten(pad_both_ends(sent, n=2) for sent in text)),sep=\"\\n\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9PjHBcWUYP4",
        "outputId": "2f913c30-6a56-49d1-a65b-2123670ecd2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Flattened sentences with padding symbols---\n",
            "<s>\n",
            "how\n",
            "are\n",
            "you\n",
            "</s>\n",
            "<s>\n",
            "i\n",
            "am\n",
            "fine\n",
            "and\n",
            "so\n",
            "is\n",
            "everything\n",
            "</s>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Value of lazy iterators - train and vocab---\")\n",
        "training_ngrams, padded_sentences = padded_everygram_pipeline(2, text)\n",
        "print(\"Unigram and bigram training iterators:\")\n",
        "for ngramlize_sent in training_ngrams:\n",
        "    print(list(ngramlize_sent),sep=\"\\n\")\n",
        "    print()\n",
        "print('#############')\n",
        "print(\"Vocabulary iterator:\")\n",
        "print(list(padded_sentences))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0IRbuxYUbF4",
        "outputId": "7d7d3cd1-42f6-4081-e8d1-b73b03420dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Value of lazy iterators - train and vocab---\n",
            "Unigram and bigram training iterators:\n",
            "[('<s>',), ('<s>', 'how'), ('how',), ('how', 'are'), ('are',), ('are', 'you'), ('you',), ('you', '</s>'), ('</s>',)]\n",
            "\n",
            "[('<s>',), ('<s>', 'i'), ('i',), ('i', 'am'), ('am',), ('am', 'fine'), ('fine',), ('fine', 'and'), ('and',), ('and', 'so'), ('so',), ('so', 'is'), ('is',), ('is', 'everything'), ('everything',), ('everything', '</s>'), ('</s>',)]\n",
            "\n",
            "#############\n",
            "Vocabulary iterator:\n",
            "['<s>', 'how', 'are', 'you', '</s>', '<s>', 'i', 'am', 'fine', 'and', 'so', 'is', 'everything', '</s>']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#   Training an N-gram model"
      ],
      "metadata": {
        "id": "V-o6I-bZUdX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
        "text = requests.get(url).content.decode('utf8')\n",
        "with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        "    fout.write(text)\n",
        "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KC0ITaOUgq5",
        "outputId": "5588405d-6409-4352-8be5-f4108b73e452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Preview of training corpus---\")\n",
        "print(print(text[:500]))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEUd0GxSU7lC",
        "outputId": "86bdffbc-b0f1-44e6-e92c-ddba39f8c30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Preview of training corpus---\n",
            "                       Language is never, ever, ever, random\n",
            "\n",
            "                                                               ADAM KILGARRIFF\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Abstract\n",
            "Language users never choose words randomly, and language is essentially\n",
            "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
            "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
            "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
            "data, we shall (almost) always be able to establish \n",
            "None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "model = MLE(n)\n",
        "print(\"------ Initializing Model ------\")\n",
        "print(\"Length of vocabulary: \", len(model.vocab))\n",
        "print(\"------ Fitting Model ------\")\n",
        "model.fit(train_data, padded_sents)\n",
        "print(model.vocab)\n",
        "print(\"Length of vocabulary: \", len(model.vocab))\n",
        "print()\n",
        "\n",
        "print(\"---Preview of training corpus---\")\n",
        "print(model.vocab.lookup(tokenized_text[0]))\n",
        "print()\n",
        "\n",
        "print(\"---Model output with unseen data---\")\n",
        "print(model.vocab.lookup('language is never random lah .'.split()))\n",
        "print()\n",
        "\n",
        "# Using the N-gram language model\n",
        "\n",
        "print(\"---Trained model with count of N-grams---\")\n",
        "print(model.counts)\n",
        "print()\n",
        "\n",
        "print(\"count('language') = \", model.counts['language'])\n",
        "print(\"count('language is') = \", model.counts[['language']]['is'])\n",
        "print(\"count('language is never') = \", model.counts[['language', 'is']]['never'])\n",
        "print()\n",
        "\n",
        "print(\"P('language') = \", model.score('language'))\n",
        "print(\"P('is' | 'language') = \", model.score('is', 'language'.split()))\n",
        "print(\"P('never' | 'language is') = \", model.score('never', 'language is'.split()))\n",
        "print()\n",
        "\n",
        "print(\"P_log('language') = \", model.logscore('language'))\n",
        "print(\"P_log('is' | 'language') = \", model.logscore('is', 'language'.split()))\n",
        "print(\"P_log('never' | 'language is') = \", model.logscore('never', 'language is'.split()))\n",
        "print()\n",
        "\n",
        "# Sentence generation using N-gram model\n",
        "\n",
        "print(\"---Sentence generated using N-gram---\")\n",
        "print(model.generate(20, random_seed=7))\n",
        "print()\n",
        "\n",
        "print(model.score(\"<UNK>\")==model.score(\"lah\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD8HBvIiVMHK",
        "outputId": "1d2411a3-f1d6-44c3-eea5-ea1eadcec35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Initializing Model ------\n",
            "Length of vocabulary:  0\n",
            "------ Fitting Model ------\n",
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1391 items>\n",
            "Length of vocabulary:  1391\n",
            "\n",
            "---Preview of training corpus---\n",
            "('language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.')\n",
            "\n",
            "---Model output with unseen data---\n",
            "('language', 'is', 'never', 'random', '<UNK>', '.')\n",
            "\n",
            "---Trained model with count of N-grams---\n",
            "<NgramCounter with 3 ngram orders and 19611 ngrams>\n",
            "\n",
            "count('language') =  25\n",
            "count('language is') =  11\n",
            "count('language is never') =  7\n",
            "\n",
            "P('language') =  0.003691671588895452\n",
            "P('is' | 'language') =  0.44\n",
            "P('never' | 'language is') =  0.6363636363636364\n",
            "\n",
            "P_log('language') =  -8.081510068120917\n",
            "P_log('is' | 'language') =  -1.1844245711374275\n",
            "P_log('never' | 'language is') =  -0.6520766965796932\n",
            "\n",
            "---Sentence generated using N-gram---\n",
            "['and', 'carroll', 'used', 'hypothesis', 'testing', 'has', 'been', 'used', ',', 'and', 'a', 'half', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
            "\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "def generate_sent(model, num_words, random_seed=42):\n",
        "    \"\"\"\n",
        "    :param model: An ngram language model from `nltk.lm.model`.\n",
        "    :param num_words: Max no. of words to generate.\n",
        "    :param random_seed: Seed value for random.\n",
        "    \"\"\"\n",
        "    content = []\n",
        "    for token in model.generate(num_words, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)\n",
        "\n",
        "print(\"---Generated sentence converted to human-readable form---\")\n",
        "print(generate_sent(model, 20, random_seed=7))\n",
        "print()\n",
        "\n",
        "print(model.vocab.lookup(tokenized_text[0]))"
      ],
      "metadata": {
        "id": "faH3GblkVVTa",
        "outputId": "3b4d8f89-aae3-4222-b54d-25b1554d08d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Generated sentence converted to human-readable form---\n",
            "and carroll used hypothesis testing has been used, and a half.\n",
            "\n",
            "('language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.')\n"
          ]
        }
      ]
    }
  ]
}